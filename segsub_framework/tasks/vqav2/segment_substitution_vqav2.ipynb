{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the correct output\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# %pip install google-generativeai\n",
    "# import google.generativeai as genai\n",
    "# from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import json \n",
    "import copy\n",
    "import re\n",
    "import time\n",
    "import cv2\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Any, Dict, List\n",
    "from sam_segment import predict_masks_with_sam_prompts\n",
    "from stable_diffusion_inpaint import fill_img_with_sd\n",
    "from utils import load_img_to_array, save_array_to_img, dilate_mask, \\\n",
    "    show_mask, show_points, get_clicked_point\n",
    "from tasks.segmentation_task import SegmentationSample, SegmentationTask\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from transformers import AutoModelForMaskGeneration, AutoProcessor, pipeline\n",
    "from PIL import Image\n",
    "import io\n",
    "from utils import load_img_to_array, save_array_to_img, format_img\n",
    "import pandas as pd\n",
    "from tasks.segmentation_task import SegmentationSample, SegmentationTask\n",
    "\n",
    "# load pyarrow\n",
    "import pandas as pd\n",
    "\n",
    "vqa_path = \"{}/data/VQAv2_arrows/vqav2_train.arrow\".format(os.path.expanduser(\"~/SegmentationSubstitution\")) \n",
    "# load the VQA dataset\n",
    "data = pd.read_feather(vqa_path)\n",
    "all_generated_files = os.listdir(\"../../results/vqa_removal_val/\")\n",
    "data.head()\n",
    "\n",
    "\n",
    "vqa_data = {}\n",
    "qid_img_q = {}\n",
    "vqa_qid_obj_dir = 'vqav2_val_obj.txt'\n",
    "output_dir = \"../../results/vqa_removal_val\"\n",
    "\n",
    "for _,row in data.iterrows():\n",
    "    img = row['image']\n",
    "    for idx,qid in enumerate(row['question_id']):\n",
    "        qid_img_q[qid] = {\"img\": img, \"q\": row['questions'][idx], \"img_id\": row['image_id']}\n",
    "\n",
    "\n",
    "with open(vqa_qid_obj_dir, 'r') as f:\n",
    "    for row in f:\n",
    "        content = row.rstrip().split('\\t')\n",
    "        assert len(content) == 2\n",
    "        qid = int(content[0])\n",
    "        if qid not in qid_img_q:\n",
    "            continue\n",
    "        llm_res = json.loads(content[1])\n",
    "        vqa_data[qid] = {\n",
    "            'object': llm_res['object'],\n",
    "            'q': qid_img_q[qid]['q'],\n",
    "            'img': qid_img_q[qid]['img'],\n",
    "            'new_label': llm_res['new_answer'],\n",
    "            'qid': qid,\n",
    "            'img_id': qid_img_q[qid]['img_id']\n",
    "        }\n",
    "\n",
    "### Load all the pipelines and models at once\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Loading SD model...\")\n",
    "infill_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-2-inpainting\",\n",
    "        torch_dtype=torch.float32,\n",
    "    ).to(device)\n",
    "print(\"Loading SAM model...\")\n",
    "detector_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "segmenter_id = \"facebook/sam-vit-base\"\n",
    "\n",
    "detector_pipe = pipeline(model=detector_id, task=\"zero-shot-object-detection\", device=device)\n",
    "segmenter = AutoModelForMaskGeneration.from_pretrained(segmenter_id).to(device)\n",
    "processor = AutoProcessor.from_pretrained(segmenter_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Dict, Optional, Union, Tuple\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from transformers import AutoModelForMaskGeneration, AutoProcessor, pipeline\n",
    "\n",
    "def get_raw_image(index):\n",
    "    return Image.open(io.BytesIO(data['image'][index])).convert(\"RGB\")\n",
    "\n",
    "def convert_image_to_binary(image):\n",
    "    with io.BytesIO() as output:\n",
    "        # Save the image to the BytesIO object in JPEG format\n",
    "        image.save(output, format='JPEG')\n",
    "        # Get the binary data from the BytesIO object\n",
    "        binary_data = output.getvalue()\n",
    "    return binary_data\n",
    "\n",
    "def get_question_from_file(file):\n",
    "    image_id = file.split(\"_\")[0]\n",
    "    row_id = data[(data['image_id'] == int(image_id))].index[0]\n",
    "    question_id = file.split(\"_\")[1].split(\".\")[0]\n",
    "    question = data['questions'][row_id][list(data['question_id'][row_id]).index(int(question_id))]\n",
    "    return question, row_id\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    xmin: int\n",
    "    ymin: int\n",
    "    xmax: int\n",
    "    ymax: int\n",
    "\n",
    "    @property\n",
    "    def xyxy(self) -> List[float]:\n",
    "        return [self.xmin, self.ymin, self.xmax, self.ymax]\n",
    "\n",
    "@dataclass\n",
    "class DetectionResult:\n",
    "    score: float\n",
    "    label: str\n",
    "    box: BoundingBox\n",
    "    mask: Optional[np.array] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, detection_dict: Dict) -> 'DetectionResult':\n",
    "        return cls(score=detection_dict['score'],\n",
    "                   label=detection_dict['label'],\n",
    "                   box=BoundingBox(xmin=detection_dict['box']['xmin'],\n",
    "                                   ymin=detection_dict['box']['ymin'],\n",
    "                                   xmax=detection_dict['box']['xmax'],\n",
    "                                   ymax=detection_dict['box']['ymax']))\n",
    "        \n",
    "def annotate(image: Union[Image.Image, np.ndarray], detection_results: List[DetectionResult]) -> np.ndarray:\n",
    "    # Convert PIL Image to OpenCV format\n",
    "    image_cv2 = np.array(image) if isinstance(image, Image.Image) else image\n",
    "    image_cv2 = cv2.cvtColor(image_cv2, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Iterate over detections and add bounding boxes and masks\n",
    "    for detection in detection_results:\n",
    "        label = detection.label\n",
    "        score = detection.score\n",
    "        box = detection.box\n",
    "        mask = detection.mask\n",
    "\n",
    "        # Sample a random color for each detection\n",
    "        color = np.random.randint(0, 256, size=3)\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image_cv2, (box.xmin, box.ymin), (box.xmax, box.ymax), color.tolist(), 2)\n",
    "        cv2.putText(image_cv2, f'{label}: {score:.2f}', (box.xmin, box.ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color.tolist(), 2)\n",
    "\n",
    "        # If mask is available, apply it\n",
    "        if mask is not None:\n",
    "            # Convert mask to uint8\n",
    "            mask_uint8 = (mask * 255).astype(np.uint8)\n",
    "            contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(image_cv2, contours, -1, color.tolist(), 2)\n",
    "\n",
    "    return cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def plot_detections(\n",
    "    image: Union[Image.Image, np.ndarray],\n",
    "    detections: List[DetectionResult],\n",
    "    save_name: Optional[str] = None\n",
    ") -> None:\n",
    "    return annotate(image, detections)\n",
    "    \n",
    "def random_named_css_colors(num_colors: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of randomly selected named CSS colors.\n",
    "\n",
    "    Args:\n",
    "    - num_colors (int): Number of random colors to generate.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of randomly selected named CSS colors.\n",
    "    \"\"\"\n",
    "    # List of named CSS colors\n",
    "    named_css_colors = [\n",
    "        'aliceblue', 'antiquewhite', 'aqua', 'aquamarine', 'azure', 'beige', 'bisque', 'black', 'blanchedalmond',\n",
    "        'blue', 'blueviolet', 'brown', 'burlywood', 'cadetblue', 'chartreuse', 'chocolate', 'coral', 'cornflowerblue',\n",
    "        'cornsilk', 'crimson', 'cyan', 'darkblue', 'darkcyan', 'darkgoldenrod', 'darkgray', 'darkgreen', 'darkgrey',\n",
    "        'darkkhaki', 'darkmagenta', 'darkolivegreen', 'darkorange', 'darkorchid', 'darkred', 'darksalmon', 'darkseagreen',\n",
    "        'darkslateblue', 'darkslategray', 'darkslategrey', 'darkturquoise', 'darkviolet', 'deeppink', 'deepskyblue',\n",
    "        'dimgray', 'dimgrey', 'dodgerblue', 'firebrick', 'floralwhite', 'forestgreen', 'fuchsia', 'gainsboro', 'ghostwhite',\n",
    "        'gold', 'goldenrod', 'gray', 'green', 'greenyellow', 'grey', 'honeydew', 'hotpink', 'indianred', 'indigo', 'ivory',\n",
    "        'khaki', 'lavender', 'lavenderblush', 'lawngreen', 'lemonchiffon', 'lightblue', 'lightcoral', 'lightcyan', 'lightgoldenrodyellow',\n",
    "        'lightgray', 'lightgreen', 'lightgrey', 'lightpink', 'lightsalmon', 'lightseagreen', 'lightskyblue', 'lightslategray',\n",
    "        'lightslategrey', 'lightsteelblue', 'lightyellow', 'lime', 'limegreen', 'linen', 'magenta', 'maroon', 'mediumaquamarine',\n",
    "        'mediumblue', 'mediumorchid', 'mediumpurple', 'mediumseagreen', 'mediumslateblue', 'mediumspringgreen', 'mediumturquoise',\n",
    "        'mediumvioletred', 'midnightblue', 'mintcream', 'mistyrose', 'moccasin', 'navajowhite', 'navy', 'oldlace', 'olive',\n",
    "        'olivedrab', 'orange', 'orangered', 'orchid', 'palegoldenrod', 'palegreen', 'paleturquoise', 'palevioletred', 'papayawhip',\n",
    "        'peachpuff', 'peru', 'pink', 'plum', 'powderblue', 'purple', 'rebeccapurple', 'red', 'rosybrown', 'royalblue', 'saddlebrown',\n",
    "        'salmon', 'sandybrown', 'seagreen', 'seashell', 'sienna', 'silver', 'skyblue', 'slateblue', 'slategray', 'slategrey',\n",
    "        'snow', 'springgreen', 'steelblue', 'tan', 'teal', 'thistle', 'tomato', 'turquoise', 'violet', 'wheat', 'white',\n",
    "        'whitesmoke', 'yellow', 'yellowgreen'\n",
    "    ]\n",
    "\n",
    "    # Sample random named CSS colors\n",
    "    return random.sample(named_css_colors, min(num_colors, len(named_css_colors)))\n",
    "\n",
    "def plot_detections_plotly(\n",
    "    image: np.ndarray,\n",
    "    detections: List[DetectionResult],\n",
    "    class_colors: Optional[Dict[str, str]] = None\n",
    ") -> None:\n",
    "    # If class_colors is not provided, generate random colors for each class\n",
    "    if class_colors is None:\n",
    "        num_detections = len(detections)\n",
    "        colors = random_named_css_colors(num_detections)\n",
    "        class_colors = {}\n",
    "        for i in range(num_detections):\n",
    "            class_colors[i] = colors[i]\n",
    "\n",
    "\n",
    "    fig = px.imshow(image)\n",
    "\n",
    "    # Add bounding boxes\n",
    "    shapes = []\n",
    "    annotations = []\n",
    "    for idx, detection in enumerate(detections):\n",
    "        label = detection.label\n",
    "        box = detection.box\n",
    "        score = detection.score\n",
    "        mask = detection.mask\n",
    "\n",
    "        polygon = mask_to_polygon(mask)\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[point[0] for point in polygon] + [polygon[0][0]],\n",
    "            y=[point[1] for point in polygon] + [polygon[0][1]],\n",
    "            mode='lines',\n",
    "            line=dict(color=class_colors[idx], width=2),\n",
    "            fill='toself',\n",
    "            name=f\"{label}: {score:.2f}\"\n",
    "        ))\n",
    "\n",
    "        xmin, ymin, xmax, ymax = box.xyxy\n",
    "        shape = [\n",
    "            dict(\n",
    "                type=\"rect\",\n",
    "                xref=\"x\", yref=\"y\",\n",
    "                x0=xmin, y0=ymin,\n",
    "                x1=xmax, y1=ymax,\n",
    "                line=dict(color=class_colors[idx])\n",
    "            )\n",
    "        ]\n",
    "        annotation = [\n",
    "            dict(\n",
    "                x=(xmin+xmax) // 2, y=(ymin+ymax) // 2,\n",
    "                xref=\"x\", yref=\"y\",\n",
    "                text=f\"{label}: {score:.2f}\",\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        shapes.append(shape)\n",
    "        annotations.append(annotation)\n",
    "\n",
    "    # Update layout\n",
    "    button_shapes = [dict(label=\"None\",method=\"relayout\",args=[\"shapes\", []])]\n",
    "    button_shapes = button_shapes + [\n",
    "        dict(label=f\"Detection {idx+1}\",method=\"relayout\",args=[\"shapes\", shape]) for idx, shape in enumerate(shapes)\n",
    "    ]\n",
    "    button_shapes = button_shapes + [dict(label=\"All\", method=\"relayout\", args=[\"shapes\", sum(shapes, [])])]\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(visible=False),\n",
    "        yaxis=dict(visible=False),\n",
    "        # margin=dict(l=0, r=0, t=0, b=0),\n",
    "        showlegend=True,\n",
    "        updatemenus=[\n",
    "            dict(\n",
    "                type=\"buttons\",\n",
    "                direction=\"up\",\n",
    "                buttons=button_shapes\n",
    "            )\n",
    "        ],\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Show plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def mask_to_polygon(mask: np.ndarray) -> List[List[int]]:\n",
    "    # Find contours in the binary mask\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Find the contour with the largest area\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Extract the vertices of the contour\n",
    "    polygon = largest_contour.reshape(-1, 2).tolist()\n",
    "\n",
    "    return polygon\n",
    "\n",
    "def polygon_to_mask(polygon: List[Tuple[int, int]], image_shape: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a polygon to a segmentation mask.\n",
    "\n",
    "    Args:\n",
    "    - polygon (list): List of (x, y) coordinates representing the vertices of the polygon.\n",
    "    - image_shape (tuple): Shape of the image (height, width) for the mask.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Segmentation mask with the polygon filled.\n",
    "    \"\"\"\n",
    "    # Create an empty mask\n",
    "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "\n",
    "    # Convert polygon to an array of points\n",
    "    pts = np.array(polygon, dtype=np.int32)\n",
    "\n",
    "    # Fill the polygon with white color (255)\n",
    "    cv2.fillPoly(mask, [pts], color=(255,))\n",
    "\n",
    "    return mask\n",
    "\n",
    "def load_image(image_str: str) -> Image.Image:\n",
    "    if image_str.startswith(\"http\"):\n",
    "        image = Image.open(requests.get(image_str, stream=True).raw).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_str).convert(\"RGB\")\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_boxes(results: DetectionResult) -> List[List[List[float]]]:\n",
    "    boxes = []\n",
    "    for result in results:\n",
    "        xyxy = result.box.xyxy\n",
    "        boxes.append(xyxy)\n",
    "\n",
    "    return [boxes]\n",
    "\n",
    "def refine_masks(masks: torch.BoolTensor, polygon_refinement: bool = False) -> List[np.ndarray]:\n",
    "    masks = masks.cpu().float()\n",
    "    masks = masks.permute(0, 2, 3, 1)\n",
    "    masks = masks.mean(axis=-1)\n",
    "    masks = (masks > 0).int()\n",
    "    masks = masks.numpy().astype(np.uint8)\n",
    "    masks = list(masks)\n",
    "\n",
    "    if polygon_refinement:\n",
    "        for idx, mask in enumerate(masks):\n",
    "            shape = mask.shape\n",
    "            polygon = mask_to_polygon(mask)\n",
    "            mask = polygon_to_mask(polygon, shape)\n",
    "            masks[idx] = mask\n",
    "\n",
    "    return masks\n",
    "\n",
    "def detect(\n",
    "    image: Image.Image,\n",
    "    labels: List[str],\n",
    "    threshold: float = 0.3,\n",
    "    detector_id: Optional[str] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use Grounding DINO to detect a set of labels in an image in a zero-shot fashion.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    detector_id = detector_id if detector_id is not None else \"IDEA-Research/grounding-dino-tiny\"\n",
    "    object_detector = pipeline(model=detector_id, task=\"zero-shot-object-detection\", device=device)\n",
    "\n",
    "    labels = [label if label.endswith(\".\") else label+\".\" for label in labels]\n",
    "\n",
    "    results = object_detector(image,  candidate_labels=labels, threshold=threshold)\n",
    "    results = [DetectionResult.from_dict(result) for result in results]\n",
    "\n",
    "    return results\n",
    "\n",
    "def segment(\n",
    "    image: Image.Image,\n",
    "    detection_results: List[Dict[str, Any]],\n",
    "    polygon_refinement: bool = False,\n",
    "    segmenter_id: Optional[str] = None\n",
    ") -> List[DetectionResult]:\n",
    "    \"\"\"\n",
    "    Use Segment Anything (SAM) to generate masks given an image + a set of bounding boxes.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    segmenter_id = segmenter_id if segmenter_id is not None else \"facebook/sam-vit-base\"\n",
    "\n",
    "    segmentator = AutoModelForMaskGeneration.from_pretrained(segmenter_id).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(segmenter_id)\n",
    "\n",
    "    boxes = get_boxes(detection_results)\n",
    "    inputs = processor(images=image, input_boxes=boxes, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = segmentator(**inputs)\n",
    "    masks = processor.post_process_masks(\n",
    "        masks=outputs.pred_masks,\n",
    "        original_sizes=inputs.original_sizes,\n",
    "        reshaped_input_sizes=inputs.reshaped_input_sizes\n",
    "    )[0]\n",
    "\n",
    "    masks = refine_masks(masks, polygon_refinement)\n",
    "\n",
    "    for detection_result, mask in zip(detection_results, masks):\n",
    "        detection_result.mask = mask\n",
    "\n",
    "    return detection_results\n",
    "\n",
    "def grounded_segmentation(\n",
    "    image: Union[Image.Image, str],\n",
    "    labels: List[str],\n",
    "    threshold: float = 0.3,\n",
    "    polygon_refinement: bool = False,\n",
    "    detector_id: Optional[str] = None,\n",
    "    segmenter_id: Optional[str] = None\n",
    ") -> Tuple[np.ndarray, List[DetectionResult]]:\n",
    "    if isinstance(image, str):\n",
    "        image = load_image(image)\n",
    "\n",
    "    detections = detect(image, labels, threshold, detector_id)\n",
    "    detections = segment(image, detections, polygon_refinement, segmenter_id)\n",
    "\n",
    "    return np.array(image), detections\n",
    "\n",
    "files = files = ['480275_480275003.jpeg', '262565_262565000.jpeg', '397587_397587000.jpeg', '266579_266579001.jpeg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = files[3]\n",
    "qid = file.split(\"_\")[1].split(\".\")[0]\n",
    "image_id = file.split(\"_\")[0]\n",
    "perturbed_path = f\"../../results/vqa_removal_val/{file}\"\n",
    "question, row_id = get_question_from_file(file)\n",
    "\n",
    "print(question)\n",
    "print(file)\n",
    "raise ValueError\n",
    "original_image = get_raw_image(row_id)\n",
    "perturbed_image = Image.open(perturbed_path)\n",
    "object = vqa_data[int(qid)]['object']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array, detections = grounded_segmentation(\n",
    "    image=original_image,\n",
    "    labels=[object],\n",
    "    threshold=0.3,\n",
    "    polygon_refinement=True,\n",
    "    detector_id=detector_id,\n",
    "    segmenter_id=segmenter_id\n",
    ")\n",
    "\n",
    "masked_original_image = Image.fromarray(plot_detections(image_array, detections))\n",
    "plt.imshow(masked_original_image)\n",
    "perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_original_image.save(f\"examples/{object}_original.png\")\n",
    "perturbed_image.save(f\"examples/{object}_removed.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQA generations\n",
    "\n",
    "# TODO: also render masks object detection side by side\n",
    "\n",
    "counterfactuals = [\n",
    "    '526197_526197006.jpeg', # no doughnuts - GPT tricked?\n",
    "    '528030_528030002.jpeg', # no necktie\n",
    "    '395717_395717003.jpeg', # no boatsman\n",
    "    '263961_263961000.jpeg', # no bike\n",
    "    '525119_525119000.jpeg', # no food on the plate\n",
    "    '437205_437205008.jpeg', # no bananas\n",
    "    '396903_396903001.jpeg', # no plane\n",
    "    '397734_397734009.jpeg', # no hotdog\n",
    "    '132415_132415003.jpeg', # no fork\n",
    "    '22461_22461002.jpeg', # no cereal\n",
    "    '264568_264568000.jpeg', # no cook\n",
    "    '262565_262565000.jpeg', # no bat - GPT tricked\n",
    "    '632_632008.jpeg', # no mirror - GPT tricked\n",
    "    '262509_262509005.jpeg', # no boat\n",
    "    '131089_131089004.jpeg', # no bat kid\n",
    "    '264375_264375000.jpeg', # no streetlight - GPT tricked?\n",
    "    '5418_5418000.jpeg', # no giraffes\n",
    "    '397587_397587000.jpeg', # no tie - GPT tricked\n",
    "    '529105_529105000.jpeg', # no horse - GPT tricked\n",
    "    '266579_266579001.jpeg', # no bird - GPT tricked\n",
    "    '267664_267664002.jpeg', # no chair\n",
    "    '480275_480275003.jpeg', # no bananas - GPT tricked\n",
    "    '393523_393523001.jpeg', # no bridge\n",
    "    '133100_133100001.jpeg', # no zebra - GPT tricked\n",
    "    '134689_134689006.jpeg', # no giraffes\n",
    "    '4175_4175000.jpeg', # no server - GPT tricked\n",
    "    '267664_267664001.jpeg', # no cat\n",
    "    '526711_526711002.jpeg', # no aircraft\n",
    "    '5385_5385010.jpeg', # no bat - GPT tricked for variant of question \"What is he holding?\", but not original question\n",
    "    '502766_502766000.jpeg', # no sheepdog. GPT not tricked despite context cues\n",
    "    '5352_5352026.jpeg', # no beer, GPT tricked\n",
    "    '393809_393809008.jpeg', # GPT tricked\n",
    "    '133343_133343002.jpeg', # no sunglasses / eyes\n",
    "    '43816_43816016.jpeg', # no mitt - GPT tricked\n",
    "    'How many different types of animals are on the field?', # no animals - GPT tricked\n",
    "    '526580_526580007.jpeg', # no jeans - GPT mini tricked\n",
    "]\n",
    "\n",
    "id = 11\n",
    "file = counterfactuals[-6]\n",
    "perturbed_path = f\"../../results/vqa_removal/{file}\"\n",
    "question, row_id = get_question_from_file(file)\n",
    "print(question)\n",
    "Image.open(perturbed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from segmentation_task import *\n",
    "\n",
    "original_image = get_raw_image(row_id)\n",
    "sample = SegmentationSample(question, None, None, image=np.array(original_image), segment_prompt=\"baseball bat\")\n",
    "perturbed_image = sample.substitution(\"broomstick\", False)\n",
    "perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['questions'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment import *\n",
    "\n",
    "def perturb_vqav2_image(segment_prompt, inpaint_prompt, index):\n",
    "\n",
    "        image_source, _, image_mask = get_frames_from_prompt(\"\", segment_prompt, model, get_raw_image(index))\n",
    "\n",
    "        # General perturbation: inpaint random replacement of same type of object\n",
    "        # Note: sometimes this borks and just removes the object totally (particularly for small objects relative to rest of image)\n",
    "        image_perturbed = inpaint_mask(inpaint_prompt, image_source, image_mask)\n",
    "        return image_perturbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from importlib import reload\n",
    "reload(webqa)\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# new df with same schema as data that will be filled with perturbed \n",
    "vqa_path = \"../data/VQAv2_arrows/vqav2_val.arrow\"\n",
    "data = pd.read_feather(vqa_path)\n",
    "perturbed_data = copy.deepcopy(data)\n",
    "dataset_root = vqa_path + \".perturbed\"\n",
    "os.makedirs(dataset_root, exist_ok=True)          \n",
    "count = 1\n",
    "for index in tqdm(range(len(data))):\n",
    "    qa_list = list(zip(data['questions'][index], [x[0] for x in data['answers'][index]]))\n",
    "    for i, (question, answer) in enumerate(qa_list):\n",
    "        if (answer.lower() in ['yes', 'no'] or question.lower().startswith(('what color', 'how many'))):\n",
    "            count += 1\n",
    "count\n",
    "            # object_noun = extract_object(question)\n",
    "            # if answer.lower() in ['yes', 'no']:\n",
    "            #     qcate = 'yesno'\n",
    "            #     if 'yes' in answer.lower():\n",
    "            #         # remove and set answer to no\n",
    "            #         infill_prompt = \"blank.png\"\n",
    "            #         rand_answer = 'no'\n",
    "            #     else:\n",
    "            #         # add and set answer to yes\n",
    "            #         infill_prompt = object_noun\n",
    "            #         rand_answer = 'yes'\n",
    "            # else:\n",
    "            #     # question.lower().startswith(('what color', 'how many')):\n",
    "            #     if question.lower().startswith('what color'):\n",
    "            #         qcate = 'color'\n",
    "            #     else:\n",
    "            #         qcate = 'number'      \n",
    "            #     rand_answer = random.choice(webqa.domain_dict_gen[qcate])\n",
    "            #     infill_prompt = rand_answer + ' ' + object_noun\n",
    "\n",
    "#             print(question, answer, infill_prompt, object_noun)\n",
    "#             perturbed_image = perturb_vqav2_image(object_noun, infill_prompt, index)\n",
    "#             new_row = copy.deepcopy(data.iloc[index])\n",
    "#             new_row['image'] = convert_image_to_binary(perturbed_image)\n",
    "#             new_row['answers'] = [rand_answer] \n",
    "#             new_row['questions'] = [question]\n",
    "#             perturbed_data = pd.concat([perturbed_data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# train_table = pa.Table.from_pandas(perturbed_data)\n",
    "  \n",
    "# # save perturbed_data as new pyarrow file \n",
    "# with pa.OSFile(f\"{dataset_root}/rand_augmented.arrow\", \"wb\") as sink:\n",
    "#         with pa.RecordBatchFileWriter(sink, train_table.schema) as writer:\n",
    "#             writer.write_table(train_table)\n",
    "\n",
    "# # remove original data\n",
    "# perturbed_data_only = perturbed_data.drop(data.index)\n",
    "# perturbed_table_only = pa.Table.from_pandas(perturbed_data_only)\n",
    "# with pa.OSFile(f\"{dataset_root}/rand_only.arrow\", \"wb\") as sink:        \n",
    "#         with pa.RecordBatchFileWriter(sink, perturbed_table_only.schema) as writer:\n",
    "#             writer.write_table(perturbed_table_only)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inpaint_lama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
